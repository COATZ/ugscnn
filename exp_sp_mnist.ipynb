{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import argparse\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable, Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fftfreqs(res, onesided=True, norm=True):\n",
    "    \"\"\"\n",
    "    Helper function to return frequency tensors\n",
    "    :param res: n_dims int tuple of number of frequency modes\n",
    "    :param t: n_dims tuple of period in each dimension\n",
    "    :param onsided (bool): onesided for real frequencies\n",
    "    :param norm (bool): normalize frequencies to 2*pi\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    n_dims = len(res)\n",
    "    freqs = []\n",
    "    for dim in range(n_dims - 1):\n",
    "        r_ = res[dim]\n",
    "        if not norm:\n",
    "            freq = np.fft.fftfreq(r_, d=1/r_)\n",
    "        else:\n",
    "            freq = np.fft.fftfreq(r_)*2*np.pi\n",
    "        freqs.append(freq)\n",
    "    r_ = res[-1]\n",
    "    if onesided:\n",
    "        if not norm:\n",
    "            freqs.append(np.fft.rfftfreq(r_, d=1/r_))\n",
    "        else:\n",
    "            freqs.append(np.fft.rfftfreq(r_)*2*np.pi)\n",
    "    else:\n",
    "        if not norm:\n",
    "            freqs.append(np.fft.fftfreq(r_, d=1/r_))\n",
    "        else:\n",
    "            freqs.append(np.fft.fftfreq(r_)*2*np.pi)\n",
    "    omega = np.meshgrid(*freqs, indexing='ij')\n",
    "    omega = list(omega)\n",
    "    omega[0], omega[1] = omega[1], omega[0]\n",
    "    omega = np.stack(omega, axis=-1)\n",
    "\n",
    "    return omega.astype(np.float32)\n",
    "\n",
    "\n",
    "class SpConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, degree=2, bias=True, device='cuda'):\n",
    "        assert degree in [0,1,2,3]\n",
    "        super(SpConv2d, self).__init__() \n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.degree = degree\n",
    "        self.device = torch.device(device)\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.Tensor(out_channels))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "#         self.ncoeff = int((self.degree + 1) * (self.degree + 2) / 2)\n",
    "        self.ncoeff = 2\n",
    "        self.coeffs = torch.Tensor(out_channels, in_channels, self.ncoeff).to(self.device)\n",
    "        self.coeffs = Parameter(self.coeffs)\n",
    "        self.set_coeffs()\n",
    "        self.ops = None\n",
    "        \n",
    "    def set_coeffs(self):\n",
    "        n = self.in_channels * self.ncoeff\n",
    "        stdv = 1. / math.sqrt(n)\n",
    "        self.coeffs.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "            \n",
    "#     def get_diff_operators(self, input):\n",
    "#         self.in_shape = list(input.size())[-2:]\n",
    "#         k = torch.tensor(fftfreqs(self.in_shape))\n",
    "#         self.k_shape = list(k.size())[:2]\n",
    "#         u, v = k[..., 0], k[..., 1]\n",
    "#         zeros = torch.zeros(*self.k_shape)\n",
    "#         ops = []\n",
    "#         # 0th order\n",
    "#         ops.append(torch.stack([torch.ones(*self.k_shape), zeros], dim=-1))\n",
    "#         if self.degree > 0:\n",
    "#             # 1st order\n",
    "#             # d/dx\n",
    "#             ops.append(torch.stack([zeros, -u], dim=-1))\n",
    "#             # d/dy\n",
    "#             ops.append(torch.stack([zeros, -v], dim=-1))\n",
    "#         if self.degree > 1:\n",
    "#             # 2nd order\n",
    "#             # d^2/dx/dy\n",
    "#             ops.append(torch.stack([-u*v, zeros], dim=-1))\n",
    "#             # d^2/dx^2\n",
    "#             ops.append(torch.stack([-u**2, zeros], dim=-1))\n",
    "#             # d^2/dx^2\n",
    "#             ops.append(torch.stack([-v**2, zeros], dim=-1))\n",
    "#         if self.degree > 2:\n",
    "#             # 3nd order\n",
    "#             # d^3/dx/dx/dx\n",
    "#             ops.append(torch.stack([zeros, u**3], dim=-1))\n",
    "#             # d^3/dx/dx/dy\n",
    "#             ops.append(torch.stack([zeros, (u**2)*v], dim=-1))\n",
    "#             # d^3/dx/dy/dy\n",
    "#             ops.append(torch.stack([zeros, u*(v**2)], dim=-1))\n",
    "#             # d^3/dy/dy/dy\n",
    "#             ops.append(torch.stack([zeros, v**3], dim=-1))\n",
    "\n",
    "#         self.ops = torch.stack(ops, dim=0).to(self.device) # shape (ncoeff, *self.k_shape, 2)\n",
    "        \n",
    "    def get_diff_operators(self, input):\n",
    "        self.in_shape = list(input.size())[-2:]\n",
    "        k = torch.tensor(fftfreqs(self.in_shape))\n",
    "        self.k_shape = list(k.size())[:2]\n",
    "        u, v = k[..., 0], k[..., 1]\n",
    "        zeros = torch.zeros(*self.k_shape)\n",
    "        ops = []\n",
    "        # I\n",
    "        ops.append(torch.stack([torch.ones(*self.k_shape), zeros], dim=-1))\n",
    "        # Nabla^2\n",
    "        ops.append(torch.stack([-u**2-v**2, zeros], dim=-1))\n",
    "# #       # Nabla^4\n",
    "#         ops.append(torch.stack([(u**2+v**2)**2, zeros], dim=-1)/10)\n",
    "        self.ncoeff = len(ops)\n",
    "        self.ops = torch.stack(ops, dim=0).to(self.device) # shape (ncoeff, *self.k_shape, 2)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        if self.ops is None:\n",
    "            self.get_diff_operators(input)\n",
    "        F_input = torch.rfft(input, 2)\n",
    "        c = self.coeffs.view(*self.coeffs.size(), 1, 1, 1)\n",
    "        self.weight = torch.sum(torch.mul(c, self.ops), dim=2)\n",
    "#         F_output = torch.sum(torch.mul(F_input.unsqueeze(1), self.weight), dim=2)\n",
    "        # convolution\n",
    "        Fr, Fi = F_input[..., 0].unsqueeze(1), F_input[..., 1].unsqueeze(1)\n",
    "        Wr, Wi = self.weight[..., 0], self.weight[..., 1]\n",
    "        out_real = torch.sum(torch.mul(Fr, Wr) - torch.mul(Fi, Wi), dim=2)\n",
    "        out_imag = torch.sum(torch.mul(Fr, Wi) + torch.mul(Fi, Wr), dim=2)\n",
    "        F_output = torch.stack([out_real, out_imag], dim=-1)\n",
    "        f_output = torch.irfft(F_output, 2, signal_sizes=self.in_shape) + self.bias.view(1, -1, 1, 1)\n",
    "        return f_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.csize = 64*7*7\n",
    "#         self.conv1 = nn.Conv2d(1, 10, kernel_size=3, padding=1)\n",
    "#         self.conv2 = nn.Conv2d(10, 20, kernel_size=3, padding=1)\n",
    "        self.conv1 = SpConv2d(1, 32, degree=2)\n",
    "        self.conv2 = SpConv2d(32, 64, degree=2)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "#         self.bn1 = nn.BatchNorm2d(32)\n",
    "#         self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.fc1 = nn.Linear(self.csize, 512)\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "#         x = F.relu(F.max_pool2d(self.bn1(self.conv1(x)), 2))\n",
    "#         x = F.relu(F.max_pool2d(self.conv2_drop(self.bn2(self.conv2(x))), 2))\n",
    "        x = x.view(-1, self.csize)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "#         print(model.conv1.coeffs.grad)\n",
    "        optimizer.step()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            sys.stdout.write('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f} \\r'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            sys.stdout.flush()\n",
    "\n",
    "def test(args, model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, size_average=False).item() # sum up batch loss\n",
    "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%) \\r'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.348444 \n",
      "Test set: Average loss: 0.2821, Accuracy: 9139/10000 (91%) \n",
      "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.498883 \n",
      "Test set: Average loss: 0.2079, Accuracy: 9393/10000 (94%) \n",
      "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.315169 \n",
      "Test set: Average loss: 0.1978, Accuracy: 9374/10000 (94%) \n",
      "Train Epoch: 4 [59520/60000 (99%)]\tLoss: 0.166760 \n",
      "Test set: Average loss: 0.1457, Accuracy: 9533/10000 (95%) \n",
      "Train Epoch: 5 [59520/60000 (99%)]\tLoss: 0.271620 \n",
      "Test set: Average loss: 0.1346, Accuracy: 9563/10000 (96%) \n",
      "Train Epoch: 6 [59520/60000 (99%)]\tLoss: 0.130100 \n",
      "Test set: Average loss: 0.1226, Accuracy: 9609/10000 (96%) \n",
      "Train Epoch: 7 [59520/60000 (99%)]\tLoss: 0.139174 \n",
      "Test set: Average loss: 0.1098, Accuracy: 9655/10000 (97%) \n",
      "Train Epoch: 8 [59520/60000 (99%)]\tLoss: 0.193953 \n",
      "Test set: Average loss: 0.1015, Accuracy: 9686/10000 (97%) \n",
      "Train Epoch: 9 [59520/60000 (99%)]\tLoss: 0.162771 \n",
      "Test set: Average loss: 0.1005, Accuracy: 9672/10000 (97%) \n",
      "Train Epoch: 10 [59520/60000 (99%)]\tLoss: 0.196373 \n",
      "Test set: Average loss: 0.0951, Accuracy: 9706/10000 (97%) \n",
      "Train Epoch: 11 [59520/60000 (99%)]\tLoss: 0.075958 \n",
      "Test set: Average loss: 0.0937, Accuracy: 9723/10000 (97%) \n",
      "Train Epoch: 12 [59520/60000 (99%)]\tLoss: 0.160146 \n",
      "Test set: Average loss: 0.0901, Accuracy: 9708/10000 (97%) \n",
      "Train Epoch: 13 [59520/60000 (99%)]\tLoss: 0.184525 \n",
      "Test set: Average loss: 0.0852, Accuracy: 9722/10000 (97%) \n",
      "Train Epoch: 14 [59520/60000 (99%)]\tLoss: 0.099735 \n",
      "Test set: Average loss: 0.0845, Accuracy: 9733/10000 (97%) \n",
      "Train Epoch: 15 [59520/60000 (99%)]\tLoss: 0.105291 \n",
      "Test set: Average loss: 0.0824, Accuracy: 9741/10000 (97%) \n",
      "Train Epoch: 16 [59520/60000 (99%)]\tLoss: 0.023047 \n",
      "Test set: Average loss: 0.0814, Accuracy: 9755/10000 (98%) \n",
      "Train Epoch: 17 [59520/60000 (99%)]\tLoss: 0.078639 \n",
      "Test set: Average loss: 0.0777, Accuracy: 9756/10000 (98%) \n",
      "Train Epoch: 18 [59520/60000 (99%)]\tLoss: 0.018746 \n",
      "Test set: Average loss: 0.0758, Accuracy: 9754/10000 (98%) \n",
      "Train Epoch: 19 [59520/60000 (99%)]\tLoss: 0.143186 \n",
      "Test set: Average loss: 0.0720, Accuracy: 9776/10000 (98%) \n",
      "Train Epoch: 20 [59520/60000 (99%)]\tLoss: 0.093427 \n",
      "Test set: Average loss: 0.0733, Accuracy: 9761/10000 (98%) \n",
      "Train Epoch: 21 [59520/60000 (99%)]\tLoss: 0.102283 \n",
      "Test set: Average loss: 0.0705, Accuracy: 9774/10000 (98%) \n",
      "Train Epoch: 22 [59520/60000 (99%)]\tLoss: 0.111925 \n",
      "Test set: Average loss: 0.0678, Accuracy: 9784/10000 (98%) \n",
      "Train Epoch: 23 [59520/60000 (99%)]\tLoss: 0.049246 \n",
      "Test set: Average loss: 0.0670, Accuracy: 9780/10000 (98%) \n",
      "Train Epoch: 24 [59520/60000 (99%)]\tLoss: 0.081499 \n",
      "Test set: Average loss: 0.0645, Accuracy: 9810/10000 (98%) \n",
      "Train Epoch: 25 [59520/60000 (99%)]\tLoss: 0.022230 \n",
      "Test set: Average loss: 0.0648, Accuracy: 9785/10000 (98%) \n",
      "Train Epoch: 26 [59520/60000 (99%)]\tLoss: 0.049359 \n",
      "Test set: Average loss: 0.0641, Accuracy: 9798/10000 (98%) \n",
      "Train Epoch: 27 [59520/60000 (99%)]\tLoss: 0.064296 \n",
      "Test set: Average loss: 0.0630, Accuracy: 9804/10000 (98%) \n",
      "Train Epoch: 28 [59520/60000 (99%)]\tLoss: 0.039253 \n",
      "Test set: Average loss: 0.0616, Accuracy: 9811/10000 (98%) \n",
      "Train Epoch: 29 [59520/60000 (99%)]\tLoss: 0.044290 \n",
      "Test set: Average loss: 0.0619, Accuracy: 9804/10000 (98%) \n",
      "Train Epoch: 30 [59520/60000 (99%)]\tLoss: 0.099411 \n",
      "Test set: Average loss: 0.0610, Accuracy: 9812/10000 (98%) \n",
      "Train Epoch: 31 [59520/60000 (99%)]\tLoss: 0.053867 \n",
      "Test set: Average loss: 0.0610, Accuracy: 9805/10000 (98%) \n",
      "Train Epoch: 32 [59520/60000 (99%)]\tLoss: 0.139746 \n",
      "Test set: Average loss: 0.0599, Accuracy: 9823/10000 (98%) \n",
      "Train Epoch: 33 [59520/60000 (99%)]\tLoss: 0.075772 \n",
      "Test set: Average loss: 0.0635, Accuracy: 9808/10000 (98%) \n",
      "Train Epoch: 34 [59520/60000 (99%)]\tLoss: 0.034558 \n",
      "Test set: Average loss: 0.0587, Accuracy: 9821/10000 (98%) \n",
      "Train Epoch: 35 [59520/60000 (99%)]\tLoss: 0.091885 \n",
      "Test set: Average loss: 0.0627, Accuracy: 9822/10000 (98%) \n",
      "Train Epoch: 36 [59520/60000 (99%)]\tLoss: 0.040938 \n",
      "Test set: Average loss: 0.0580, Accuracy: 9818/10000 (98%) \n",
      "Train Epoch: 37 [59520/60000 (99%)]\tLoss: 0.072663 \n",
      "Test set: Average loss: 0.0594, Accuracy: 9830/10000 (98%) \n",
      "Train Epoch: 38 [59520/60000 (99%)]\tLoss: 0.077740 \n",
      "Test set: Average loss: 0.0590, Accuracy: 9811/10000 (98%) \n",
      "Train Epoch: 39 [59520/60000 (99%)]\tLoss: 0.033974 \n",
      "Test set: Average loss: 0.0577, Accuracy: 9825/10000 (98%) \n",
      "Train Epoch: 40 [59520/60000 (99%)]\tLoss: 0.015507 \n",
      "Test set: Average loss: 0.0593, Accuracy: 9823/10000 (98%) \n",
      "Train Epoch: 41 [59520/60000 (99%)]\tLoss: 0.219806 \n",
      "Test set: Average loss: 0.0589, Accuracy: 9813/10000 (98%) \n",
      "Train Epoch: 42 [59520/60000 (99%)]\tLoss: 0.027925 \n",
      "Test set: Average loss: 0.0558, Accuracy: 9834/10000 (98%) \n",
      "Train Epoch: 43 [59520/60000 (99%)]\tLoss: 0.118772 \n",
      "Test set: Average loss: 0.0538, Accuracy: 9848/10000 (98%) \n",
      "Train Epoch: 44 [59520/60000 (99%)]\tLoss: 0.088513 \n",
      "Test set: Average loss: 0.0584, Accuracy: 9828/10000 (98%) \n",
      "Train Epoch: 45 [59520/60000 (99%)]\tLoss: 0.044462 \n",
      "Test set: Average loss: 0.0592, Accuracy: 9815/10000 (98%) \n",
      "Train Epoch: 46 [59520/60000 (99%)]\tLoss: 0.074516 \n",
      "Test set: Average loss: 0.0563, Accuracy: 9833/10000 (98%) \n",
      "Train Epoch: 47 [59520/60000 (99%)]\tLoss: 0.028305 \n",
      "Test set: Average loss: 0.0572, Accuracy: 9823/10000 (98%) \n",
      "Train Epoch: 48 [59520/60000 (99%)]\tLoss: 0.012086 \n",
      "Test set: Average loss: 0.0597, Accuracy: 9825/10000 (98%) \n",
      "Train Epoch: 49 [59520/60000 (99%)]\tLoss: 0.054390 \n",
      "Test set: Average loss: 0.0572, Accuracy: 9841/10000 (98%) \n",
      "Train Epoch: 50 [59520/60000 (99%)]\tLoss: 0.018494 \n",
      "Test set: Average loss: 0.0592, Accuracy: 9816/10000 (98%) \n"
     ]
    }
   ],
   "source": [
    "# Training settings\n",
    "parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n",
    "parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
    "                    help='input batch size for training (default: 64)')\n",
    "parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n",
    "                    help='input batch size for testing (default: 1000)')\n",
    "parser.add_argument('--epochs', type=int, default=50, metavar='N',\n",
    "                    help='number of epochs to train (default: 10)')\n",
    "parser.add_argument('--lr', type=float, default=1e-2, metavar='LR',\n",
    "                    help='learning rate (default: 0.01)')\n",
    "parser.add_argument('--momentum', type=float, default=0.5, metavar='M',\n",
    "                    help='SGD momentum (default: 0.5)')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='disables CUDA training')\n",
    "parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                    help='random seed (default: 1)')\n",
    "parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "                    help='how many batches to wait before logging training status')\n",
    "args = parser.parse_args('')\n",
    "use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "#                        transforms.RandomRotation(180),\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "#                        transforms.RandomRotation(180),\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,)),\n",
    "                   ])),\n",
    "    batch_size=args.test_batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "model = Net().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    train(args, model, device, train_loader, optimizer, epoch)\n",
    "    test(args, model, device, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "c1 = model.conv1.coeffs.detach().cpu().detach().numpy().reshape(-1, 10)\n",
    "c2 = model.conv2.coeffs.detach().cpu().detach().numpy().reshape(-1, 10)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(np.absolute(c1))\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(5, 30\n",
    "                   ))\n",
    "plt.imshow(np.absolute(c2))\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.bar(np.arange(c1.shape[1]), np.absolute(c1).mean(0))\n",
    "plt.show()\n",
    "plt.figure()\n",
    "plt.bar(np.arange(c2.shape[1]), np.absolute(c2).mean(0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = dict(model.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in params.keys():\n",
    "    print(k, \"shape: \", params[k].shape)\n",
    "#     print(params[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "from torchviz import make_dot, make_dot_from_trace\n",
    "\n",
    "for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    break\n",
    "output = model(data)\n",
    "make_dot(output, params=dict(model.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
